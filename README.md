# End-to-End Customer Satisfaction MLOps Pipeline

This project builds a production-ready MLOps pipeline to predict customer satisfaction scores for e-commerce orders. It uses [ZenML](https://zenml.io/) to orchestrate the pipeline, [MLflow](https://mlflow.org/) for experiment tracking, and [Streamlit](https://streamlit.io/) for the user interface.

Video Demo [Youtube Link](https://youtu.be/OniocHxiQnk)

## ðŸš€ Project Overview

The pipeline consists of the following steps:
1.  **Ingest Data:** Loads the customer order dataset.
2.  **Clean Data:** Preprocesses the data and splits it into training and testing sets.
3.  **Train Model:** Trains a machine learning model (e.g., LightGBM/XGBoost).
4.  **Evaluate Model:** Calculates metrics like MSE and R2 Score.
5.  **Deployment Trigger:** Checks if the model meets the accuracy threshold (MSE <= 2.0).
6.  **Deploy/Predict:**
    * **Local Mode:** Loads the model artifact directly for batch inference.
    * **Docker Mode:** Deploys the model as a REST API service using MLflow.

## ðŸ› ï¸ Prerequisites

* Python 3.8+
* [Docker Desktop](https://www.docker.com/products/docker-desktop/) (Only required for Docker Deployment)
* Git

## ðŸ“¦ Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/your-username/customer-satisfaction-mlops.git](https://github.com/your-username/customer-satisfaction-mlops.git)
    cd customer-satisfaction-mlops
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    python -m venv .venv
    # Windows
    .venv\Scripts\Activate.ps1
    # Mac/Linux
    source .venv/bin/activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    zenml integration install mlflow sklearn
    ```

---

## ðŸƒâ€â™€ï¸ Option 1: Local Deployment (Recommended for Windows)

This method runs the entire pipeline natively in Python without requiring Docker containers. It is faster for development and debugging.

### 1. Setup the Stack
Initialize ZenML and set up a local stack with MLflow tracking.

```bash
zenml init
zenml stack register local_mlflow_stack -o default -a default -e mlflow_tracker
zenml stack set local_mlflow_stack


# 2. Run the Pipeline

You can run training, inference, or both using the following commands:

## Train the Model

```bash
python run_local_deployment.py --config deploy


Run Inference (Predict)

```bash
python run_local_deployment.py --config predict


## Train & Predict in One Go

```bash
python run_local_deployment.py --config deploy_and_predict


 Run the Streamlit App

'''bash
streamlit run streamlit_app.py

##  Option 2:  Docker Deployment (Production)
This method deploys the model as a live MLflow prediction server inside a Docker container.

### 1. Prerequisites
### Ensure Docker Desktop is running.

Windows Users Note: If you encounter connection issues, stick to Option 1 or ensure you are using the default Named Pipe connection (npipe://).

### 2. Setup the Stack
Register the MLflow Model Deployer:

'''bash
zenml model-deployer register mlflow_deployer --flavor=mlflow

'''bash
zenml stack register docker_stack -o default -a default -e mlflow_tracker -d 
mlflow_deployer

'''bash
zenml stack set docker_stack



### 3. Run the Deployment

'''bash
python run_deployment.py --config deploy


Monitoring & Dashboard

Launch the Dashboard

'''bash
zenml up

Then open your browser at http://127.0.0.1:8237.


## Dashboard Features
Pipelines: View the DAG (Directed Acyclic Graph) of your training and inference runs

Runs: Check the status and logs of each step

Artifacts: Inspect the data and model artifacts generated by your pipelines

## Project Structure

â”œâ”€â”€ data/                       # Dataset files
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ deployment_pipeline.py       # Docker-based pipeline definition
â”‚   â””â”€â”€ local_deployment_pipeline.py # Local python-based pipeline definition
â”œâ”€â”€ steps/
â”‚   â”œâ”€â”€ clean_data.py           # Data cleaning step
â”‚   â”œâ”€â”€ evaluation.py           # Model evaluation step
â”‚   â”œâ”€â”€ ingest_data.py          # Data ingestion step
â”‚   â”œâ”€â”€ model_train.py          # Model training step
â”‚   â””â”€â”€ config.py               # Configuration parameters
â”œâ”€â”€ run_deployment.py           # Script to run Docker deployment
â”œâ”€â”€ run_local_deployment.py     # Script to run Local deployment
â”œâ”€â”€ streamlit_app.py            # Streamlit frontend application
â””â”€â”€ requirements.txt            # Python dependencies